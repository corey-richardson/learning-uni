# Introduction to Parallel Programming

- [what-is-parallel-computing](#what-is-parallel-computing)
    - [serial-computation](#serial-computation)
    - [parallel-computation](#parallel-computation)
- [high-performance-computing-hpc](#high-performance-computing-hpc)
    - [market-overview](#market-overview)
- [evolution-of-multi-core-processors](#evolution-of-multi-core-processors)
    - [pre-2006-paradigm](#pre-2006-paradigm)
    - [hardware-evolution-in-computing](#hardware-evolution-in-computing)
    - [era-of-parallel-computing](#era-of-parallel-computing)
- [heterogeneuous-computing](#heterogeneuous-computing)
    - [software-issues-in-heterogeneous-computing](#software-issues-in-heterogeneous-computing)
- [hpc-programming-languages](#hpc-programming-languages)
    - [most-commonly-used-languages-in-hpc](#most-commonly-used-languages-in-hpc)
    - [models-frameworks-and-libraries](#models-frameworks-and-libraries)
        - [openmp-open-multi-processing](#openmp-open-multi-processing)
        - [mpi-message-passing-interface](#mpi-message-passing-interface)
        - [cuda-compute-unified-device-architecture](#cuda-compute-unified-device-architecture)
        - [opencl-open-computing-language](#opencl-open-computing-language)
        - [openacc-open-accelerators](#openacc-open-accelerators)
    - [design-patterns](#design-patterns)
        - [single-program-multiple-data-spmd-pattern](#single-program-multiple-data-spmd-pattern)
        - [loop-parallelism-pattern](#loop-parallelism-pattern)
        - [task-parallelism-pattern](#task-parallelism-pattern)
        - [divide-and-conquer-pattern](#divide-and-conquer-pattern)
- [what-is-openmp](#what-is-openmp)
    - [compiler-directives](#compiler-directives)
    - [library-routines](#library-routines)
    - [environment-variables](#environment-variables)
    - [advantages-of-openmp](#advantages-of-openmp)
    - [limitations-of-openmp](#limitations-of-openmp)
    - [serial-vs-openmp-example](#serial-vs-openmp-example)
    - [vector-addition-example-using-openmp-openacc-and-cuda](#vector-addition-example-using-openmp-openacc-and-cuda)
- [graphics-processing-units-gpu](#graphics-processing-units-gpu)
    - [cpus-and-gpus](#cpus-and-gpus)
- [exascale-hardware-architectures](#exascale-hardware-architectures)

---

## What is Parallel Computing?

Parallel computing is a computational approach in which multiple tasks or calculations are performed simultaneously by dividing a problem into smaller parts. Each part is processed concurrently, utilising multiple hardware resources like CPUs, GPUs, or distributed systems.

### Serial Computation

- Code is developed in a high level language
- Code Instructions are executed one after another
- 1 CPU runs the code

### Parallel Computation

Multiple computations or processes run at the same time. Involves splitting a task into smaller, independent or semi-independent sub-tasks. Uses multiple processing units (e.g., multiple cores in a CPU or a combination of CPUs and GPUs) to execute tasks concurrently. Parallel computing scales well with the availability of additional processing resources.

1. Decomposition
    - Break down a large problem into smaller, manageable sub-problems.
2. Task Allocation
    - Assign sub-tasks to multiple processors or computing units.
3. Synchronisation
    - Ensure tasks running in parallel coordinate and share data where necessary.
4. Aggregation
    - Combine the results of the individual sub-tasks to form the final solution.

Image Processing: A large image is divided into smaller sections, and each section is processed simultaneously by different processors.

Weather Prediction: Large datasets of meteorological information are divided and analysed concurrently to produce faster and more accurate forecasts.

Scientific Simulations: Weather modeling, fluid dynamics, and physics simulations.

Data Analysis: Big data processing and ML.

Real-Time Systems: Video rendering, gaming and AR/VR.

Not all problems are easily divided into independent tasks. Introduces overhead; synchronisation and data sharing can introduce delays.

## High Performance Computing (HPC)

### Market Overview

The HPC Market Map illustrates the increasing role of high-performance computing in enhancing industrial competitiveness across the UK and Europe. HPC is a key enabler of innovation, making it indispensable in industries ranging from manufacturing to healthcare and finance.

Efficient software is the backbone of modern computing systems. Whether in small embedded devices, large-scale supercomputers, or expansive data centres, optimised software ensures reliable performance, energy efficiency, and scalability.

The industry demands professionals capable of developing efficient, high-performance software. Engineers with expertise in HPC and efficient software development are highly sought after in the industry.

HPC systems consume significant power, making energy efficiency a growing priority. Engineers are expected to innovate solutions that optimise power usage without compromising performance.

Beyond coding, engineers must understand:
- Parallel programming and distributed systems
- Hardware acceleration technologies like GPUs and FPGAs
- Software profiling and debugging tools tailored for HPC environments

## Evolution of Multi-Core Processors

### Pre-2006 Paradigm

> Why Is the Trend Moving Towards More CPU Cores?

Since 2006, the computing industry has observed a significant shift towards integrating more cores into CPUs in devices such as PCs, laptops, and servers. This trend reflects the need to overcome physical and technical limitations in traditional CPU design.

Before 2006, performance improvements were primarily achieved by increasing CPU clock speeds (frequency).
- Power Consumption and Heat: Higher frequencies led to exponential increases in power usage and heat dissipation, making further scaling unsustainable.
- Energy Efficiency: Higher clock speeds yielded diminishing returns on performance relative to energy consumption.

*Dennard Scaling* - the principle that transistor power density remains constant as transistors shrink - began to break down due to leakage currents and heat dissipation issues in smaller transistors, further limiting the ability to increase clock speeds.

The industry responded by adopting multi-core architectures, distributing workloads across multiple processing units instead of relying solely on clock speed.
- Improved parallelism, where tasks are divided across cores for faster execution
- Better power efficiency, as lower-frequency cores working in parallel consume less power than a single high-frequency core

The power of a processor is given by
$$\text{Power} = \text{Capacitance} * \text{Voltage} * \text{Frequency}^2$$

By using two processors inside the same chip, with half the frequency each:
$$\text{Capacitance}_2 = 2.2 * c$$
$$\text{Frequency}_2 = \frac{f}{2}$$
$$\text{Voltage}_2 = 0.6 * v$$
$$\text{Power}_2 = 0.4 * p$$

***Parallel computing gives us the ability to give the same performance with lower power.***

### Hardware Evolution in Computing

Over the years, computer hardware has evolved significantly to deliver enhanced performance, efficiency, and scalability. This evolution reflects a series of innovations aimed at addressing increasing computational demands while overcoming physical and architectural limitations.

- Scalar Processors
- Pipelined Processors
- Superscalar Processors
- Out-of-order Processors
- Vectorization
- Hyper-Threading (Simultaneous Multithreading)
- Multicore Processors
- Manycore Processors
- Heterogeneous Systems
    - Combine different types of processors (e.g., CPUs, GPUs, and specialised accelerators) to optimise performance for diverse tasks.

### Era of Parallel Computing

In today's computing landscape, hardware advancements alone are no longer sufficient to achieve significant performance gains. The responsibility has shifted to software developers to optimise applications and fully utilise the capabilities of parallel hardware.

While tools exist to assist in parallelising code, there are currently no tools advanced enough to automatically and optimally convert serial (single-threaded) software into parallel applications that can fully exploit modern multi-core and parallel hardware.

In the past, performance gains came effortlessly as hardware manufacturers increased CPU clock speeds. However, with the plateauing of clock speeds and the rise of multi-core processors, performance improvements now require active effort in software design and optimisation.

Multi-core CPUs, GPUs, and specialised accelerators (e.g., TPUs, FPGAs) dominate modern computing. Writing parallel applications ensures software can effectively leverage these resources.

## Heterogeneuous Computing

Heterogeneous computing involves systems that incorporate different types of processors or cores to execute specific tasks more efficiently. Unlike traditional systems that rely solely on identical processors, heterogeneous systems include dissimilar (co-)processors, each designed to optimise specific workloads.

GPPs
- Handle a wide range of tasks and provide flexibility.
- Examples: Intel, AMD CPUs.

Graphics Processing Units (GPUs)
- Optimised for highly parallel tasks, such as graphics rendering and AI workloads.
- Examples: NVIDIA CUDA cores, AMD Radeon GPUs.

Digital Signal Processors (DSPs)
- Specialised for real-time signal processing, such as audio and image processing.
- Examples: Qualcomm Hexagon DSPs

Application-Specific Instruction Set Processors (ASIPs)
- Custom processors tailored for specific applications or industries.
- Examples: Network packet processing or IoT devices.

Other Accelerators
- Tensor Processing Units (TPUs) for AI tasks.
- Field-Programmable Gate Arrays (FPGAs) for flexible, hardware-accelerated processing.

Specialised processors excel at specific tasks, improving execution speed. GPUs accelerate matrix computations for AI models. DSPs process multimedia streams in real-time.

Tasks are assigned to the most suitable processor, reducing unnecessary power consumption. Using low-power DSPs for audio decoding instead of a high-power CPU.

Heterogeneous systems can handle diverse workloads simultaneously, enhancing multitasking capabilities. Running AI inference on a GPU while the CPU handles general application logic.

Developers must manage workload distribution between processors, requiring knowledge of specialised programming frameworks like OpenCL, CUDA,

### Software Issues in Heterogeneous Computing

While heterogeneous computing systems offer substantial benefits in terms of performance and energy efficiency, they also introduce several challenges that must be addressed to fully leverage these advantages.

Offloading
- Offloading refers to the process of transferring specific tasks from a general-purpose processor (CPU) to a specialised processor (e.g., GPU, FPGA, or DSP) for more efficient execution.
- Task Identification: Not all tasks can benefit from offloading. Developers must determine which parts of the application are best suited for specialised hardware.
- Data Transfer Overhead: Offloading tasks often requires transferring data between processors, which can incur significant latency and overhead. Ensuring efficient data movement is crucial to maintaining performance gains.
- Synchronization: Managing the interaction between multiple processors (e.g., ensuring the CPU and GPU are working in harmony) can add complexity to the software design.

Programmability
- Programming for heterogeneous systems requires different approaches and languages depending on the type of processor or accelerator used.
- CPU-based programming is relatively straightforward with well-established tools and libraries. However, as systems become more complex (e.g., multi-core or multi-processor systems), programmers need to manage parallelism explicitly using multi-threading, OpenMP, or other parallel programming techniques.
- GPUs are highly parallel, and programming them requires using GPU-specific languages such as CUDA (for NVIDIA GPUs) or OpenCL (for a range of platforms). These languages allow developers to write code that can execute many operations simultaneously. However, CUDA and OpenCL have steep learning curves, particularly for developers familiar only with traditional CPU programming.
- FPGAs offer custom hardware configurations, and programming them typically involves hardware description languages like VHDL or Verilog. Writing FPGA code can be more complex than writing for CPUs or GPUs because it requires thinking in terms of hardware-level logic, rather than software functions.

Portability
- Portability refers to the ability of software to run across different heterogeneous systems without requiring significant rework. This is an important issue, as different systems may have distinct processors or accelerators (e.g., CPUs, GPUs, or FPGAs), and the same code must work across these diverse architectures.
- *What happens if your code runs on a machine with an FPGA instead of a GPU?*
- Code written for a specific type of processor (e.g., CUDA for GPUs) will not easily port to another type of processor (e.g., VHDL for FPGAs). This could require significant rewrites or adaptations, as the programming models and hardware capabilities are different.
- To improve portability, developers often rely on cross-platform libraries and frameworks such as OpenCL or Vulkan that can abstract some of the differences between hardware architectures. However, even with these frameworks, optimising code for each hardware type still requires effort.
- Even within the same type of hardware (e.g., two different GPUs), performance can vary based on specific features, capabilities, and even driver versions. This makes achieving true portability across different systems difficult.

## HPC Programming Languages

High Performance Computing (HPC) focuses on achieving the highest possible performance from computing systems, often dealing with complex simulations, data analysis, and scientific computing.

To fully utilise the capabilities of modern HPC systems, it is essential to choose the right programming languages, as they directly impact the efficiency and scalability of applications.

### Most Commonly Used Languages in HPC

C and C++ are well-known for their close-to-hardware execution and fine-grained control over memory and system resources. These languages allow developers to write highly optimised code that can directly take advantage of the processor's capabilities, making them ideal for performance-critical applications.

C and C++ provide low-level control over system resources like memory allocation and management, which is crucial for optimising performance in large-scale applications.

C/C++ support parallel programming through libraries such as OpenMP (for shared memory) and MPI (for distributed memory), which are commonly used in HPC environments.

Fortran has a long history in scientific and engineering applications, with many large-scale legacy systems (e.g., weather forecasting models like those used by the MetOffice) still in active use. Many HPC simulations and applications are written in Fortran due to its long-standing adoption in high-performance domains.

Fortran is highly optimised for numerical computation and matrix operations, which makes it ideal for tasks involving heavy mathematical computations, such as fluid dynamics, climate modelling, and physical simulations.

Fortran includes parallelism support through features like Fortran 90+ (coarrays), OpenMP, and MPI, allowing it to scale across multiple processors or distributed systems.

Fortran compilers are highly optimised, particularly for numerical workloads, which ensures excellent performance in scientific applications.

Although not as performance-focused as C/C++ or Fortran, Python is increasingly used in HPC due to its ease of use and the availability of powerful libraries like NumPy, SciPy, and Dask, which leverage underlying C/C++ code for performance. Python is often used for scripting, data processing, and as a high-level interface to more performance-oriented languages.

### Models, Frameworks and Libraries

> OpenMP, MPI, OpenCL, OpenACC, CUDA

- Ease of Use
- Performance
- Portability

#### OpenMP (Open Multi-Processing)

- A widely used framework for shared-memory parallelism. It allows developers to parallelise applications using directives (pragmas) added to standard C, C++, and Fortran code.
- Relatively easy to learn and integrate into existing codebases.
- Scientific computing, simulations, and applications running on multi-core systems.

#### MPI (Message Passing Interface)

- A framework designed for distributed-memory systems, enabling communication between processes running on separate nodes in a cluster.
- Complex to learn and implement due to the need for explicit management of data communication.
- Large-scale HPC applications like weather forecasting, molecular dynamics, and fluid dynamics.

#### CUDA (Compute Unified Device Architecture)

- A parallel programming model and API specifically designed for NVIDIA GPUs.
- Requires familiarity with GPU programming and understanding of hardware-level optimisations.
- Excellent performance for highly parallel tasks like matrix computations, AI training, and image processing.
- Machine learning, deep learning, and GPU-intensive simulations.

#### OpenCL (Open Computing Language)

- A cross-platform framework for writing programs that execute across heterogeneous systems, including CPUs, GPUs, FPGAs, and other processors.
- More complex than CUDA due to its generalised nature, but offers flexibility.
- General-purpose computing on GPUs (GPGPU), heterogeneous computing, and applications requiring portability.

#### OpenACC (Open Accelerators)

- A directive-based model for parallel programming on GPUs and other accelerators, similar to OpenMP but targeted at heterogeneous systems.
- Simplifies GPU programming by abstracting hardware details.
- Scientific applications, simulations, and tasks requiring GPU acceleration without deep hardware-specific coding.

### Design Patterns

Parallel programming, regardless of the programming language or framework, relies on a set of universal algorithmic concepts to organise and optimise computations across multiple processors or cores. These concepts, known as design patterns for parallel programming, help developers structure parallel algorithms effectively for scalability and performance.

#### Single Program Multiple Data (SPMD) Pattern

A single program is executed across multiple processing elements (e.g., threads, cores, nodes), with each element operating on its own portion of the data.

Often used in distributed memory systems (e.g., MPI applications). Each processing unit may have its own local memory.

Useful for parallel matrix multiplication, where each processing element computes a subset of the output matrix.

#### Loop Parallelism Pattern

A common pattern where iterations of a loop are distributed among multiple processors or cores. Each core executes a subset of the loop's iterations independently, reducing execution time.

Highly applicable in shared-memory systems using frameworks like OpenMP. Requires minimal changes to serial code (e.g., adding `#pragma omp parallel for`).

Useful for simulating particle movement in physics, where each particle's position is updated in parallel.

#### Task Parallelism Pattern

Different tasks or functions are distributed across multiple processors or cores, with each executing a distinct part of the workload. Tasks may run independently or communicate with one another.

Useful when tasks are heterogeneous and require different amounts of computation. Ideal for multi-threading frameworks or task-based libraries like Intel TBB.

Used in video processing, where one task handles decoding, another handles rendering, and a third handles analysis.

#### Divide and Conquer Pattern

A problem is recursively divided into smaller sub-problems until they become small enough to solve directly. The solutions to the sub-problems are then combined to form the final result.

Naturally fits recursive algorithms and can leverage parallelism during both the divide and conquer stages. Efficiently maps to multi-core and distributed systems.

Useful for parallel sorting algorithms like parallel QuickSort or MergeSort.

## What is OpenMP?

OpenMP (Open Multi-Processing) is a widely-used Application Programming Interface (API) that simplifies the development of multithreaded applications. 

It provides a straightforward way to implement parallelism in C, C++, and Fortran programs using compiler directives, library routines, and environment variables.

OpenMP allows developers to quickly convert sequential programs into parallel ones by adding simple directives (pragmas) to their code. It is ideal for shared-memory systems and requires minimal modifications to the original code.

OpenMP enables developers to create multithreaded applications where multiple threads execute concurrently, sharing the same memory space. Threads are managed internally by OpenMP, removing the need for explicit thread creation and management.

Supported in C, C++, and Fortran, making it versatile for scientific computing and general-purpose programming. Many compilers, such as GCC, Intel, and Clang, support OpenMP.

OpenMP uses the fork-join model for parallel execution:
- The program starts as a single thread (the master thread).
- At specific points marked by OpenMP directives, the master thread spawns multiple threads (fork).
- These threads execute in parallel, and once they complete their tasks, they join back into the master thread (join).

### Compiler Directives

Special pragmas or annotations in the code that guide the compiler to parallelise specific sections.

```c
#pragma omp parallel for
for (int i = 0; i < n; i++) 
{
    // Code executed by multiple threads
}
```

### Library Routines

Functions provided by OpenMP to control threading, synchronisation, and runtime behaviour.

- `omp_get_thread_num()`: Returns the thread ID of the calling thread.
- `omp_set_num_threads(n)`: Sets the number of threads for parallel regions.

### Environment Variables

Variables to control OpenMP behaviour without modifying code.

- `OMP_NUM_THREADS`: Specifies the number of threads to be used.

### Advantages of OpenMP

- Simplifies Parallel Programming: Reduces the complexity of thread management.
- Portability: Works on a wide range of shared-memory architectures.
- Incremental Parallelisation: Allows step-by-step parallelisation of an existing sequential program.
- Flexibility: Supports dynamic adjustments, such as thread numbers and workload distribution.

### Limitations of OpenMP

- Shared Memory Requirement: Only works efficiently on shared-memory architectures, limiting scalability for distributed systems.
- Overheads: Thread creation and synchronisation introduce some runtime overhead.
- Explicit Data Management: Requires careful handling of shared and private data to avoid race conditions.

### Serial vs OpenMP Example

> The provided code calculates an approximation of the mathematical constant π (pi) using the numerical integration technique, specifically the Riemann sum method. This technique approximates the area under the curve of a function, which in this case is used to compute the value of π by integrating the formula for a quarter-circle.

```c
// Serial

double un_opt() 
{
    int i;
    double x, pi, sum = 0.0;
    double step = 1.0 / (double)num_steps;

    for (i = 0; i < num_steps; i++) {
        x = (i + 0.5) * step;
        sum += 4.0 / (1.0 + x * x);
    }

    pi = step * sum;
    return pi;
}
```

```c
// OpenMP

double opt() {
    int i;
    double x, pi, sum = 0.0;
    double step;

    // Calculate the step size
    step = 1.0 / (double)num_steps;

    // Parallelised loop with OpenMP
    #pragma omp parallel for private(x) reduction(+:sum)
    for (i = 0; i < num_steps; i++) {
        x = (i + 0.5) * step; // Midpoint of the interval
        sum += 4.0 / (1.0 + x * x); // Accumulate area of rectangles
    }

    // Calculate the final approximation of pi
    pi = step * sum;

    return pi;
}
```

### Vector Addition Example using OpenMP, OpenACC and CUDA

CUDA is designed just for Nvidia GPUs, CUDA code is at a lower level, better control of the hardware resources, allows for code optimizations. 

OpenACC is a directive-based approach designed for offloading computations to accelerators like GPUs. It is a higher-level API compared to CUDA, making it easier to port applications to GPUs with minimal code changes.

OpenMP also supports offloading to GPUs with its target directive, enabling both CPU and GPU parallelism with minimal code changes. It offers a more flexible approach than OpenACC, allowing developers to fine-tune performance.

CUDA is a low-level parallel programming model specific to NVIDIA GPUs, offering direct control over hardware resources. This gives the programmer more flexibility to optimise performance but requires more detailed knowledge of GPU architecture.

```c
// OpenACC code that runs on the GPU
#pragma acc kernels copyout(c[0:n]) copyin(a[0:n], b[0:n])
for (i=0; i<n; i++) 
{
    c[i] = a[i] + b[i];
}

// OpenMP code that runs on the GPU
#pragma omp target map(to: a[0:N], b[:N]) map(from: c[0:N])
#pragma omp parallel for
for (int i = 0; i < N; i++) 
{
    c[i] = a[i] + b[i];
}
```

```c
// CUDA code that runs on the GPU
void sin_serial(const float* in, float* out) 
{
    int i;

    for (i = 0; i < N; i++)
    {
        out[i] = sinf(in[i]);
    }
}

__global__ void sin_parallel (const float* in, float* out) 
{
    int g_id = threadIdx.x + blockIdx.x * blockDim.x;

    if (g_id < N) 
    {
        out[g_id] = sinf(in[g_id]);
    }
}
```

OpenACC and OpenMP are higher-level, directive-based approaches that allow for easier offloading of computations to GPUs with minimal changes to existing code. They are ideal for applications where you want to accelerate computations with less concern for hardware-specific details.

CUDA, on the other hand, offers fine-grained control over the GPU, enabling deeper optimisation and better performance, but it requires a detailed understanding of the hardware and parallel programming concepts like thread management, memory hierarchy, and kernel launches. CUDA is best suited for performance-critical applications where maximum control is necessary.

## Graphics Processing Units (GPU)

Graphics Processing Units (GPU) were originally designed to accelerate the large number of multiply and add computations performed in graphics rendering.

This computational pattern closely resembles the workloads found in many engineering and scientific simulations, such as numerical modelling, physics simulations, and weather forecasting.

High Throughput
- GPUs consist of thousands of smaller cores, making them ideal for data-parallel workloads.
- They excel at performing repetitive operations across large datasets, such as matrix multiplications in machine learning and image/video processing.

Modern GPUs are tailored for high-performance computing, supporting double-precision floating-point operations critical for scientific simulations.

- Machine Learning: Training neural networks and performing inference.
- Image and Video Processing: Filters, transformations, and encoding/decoding.
- Scientific Simulations: Molecular dynamics, weather models, and finite element analysis.

GPUs provide significant computational power at a fraction of the cost of large-scale CPU-based systems.

### CPUs and GPUs

CPU (Central Processing Unit)
- Optimised for Serial Tasks: CPUs are designed for general-purpose computing with a small number of powerful cores that are efficient at executing complex instructions sequentially.
- Low Latency: Ideal for tasks that require fast decision-making or high levels of control (e.g., operating systems, database queries).

GPU (Graphics Processing Unit)
- Optimised for Parallel Tasks: GPUs have thousands of smaller, less powerful cores designed to handle thousands of parallel threads simultaneously.
- High Throughput: Best suited for workloads that involve repetitive and parallel operations, such as matrix computations or pixel shading.

## Exascale Hardware Architectures

Exascale computing is poised to revolutionise computational science and engineering, offering 1000x the performance of current systems while maintaining similar power consumption levels. This leap in computational capability will enable new possibilities in a variety of fields, such as climate modelling, artificial intelligence, and molecular simulations. 

However, fully leveraging this massive increase in performance requires careful consideration of both hardware architecture and software scalability.

Exascale systems are expected to employ heterogeneous hardware architectures, combining multiple types of processors to optimise performance for different types of workloads.

CPUs + GPUs (Aurora)
- CPUs provide general-purpose computing capabilities, while GPUs (Graphics Processing Units) are highly optimised for parallel computation, especially tasks like simulations, machine learning, and image processing.
- An example is the Aurora supercomputer, which will leverage the combined power of CPUs and GPUs to achieve exascale performance.

CPUs + FPGAs (Arm EPI)
- FPGAs (Field-Programmable Gate Arrays) are reconfigurable hardware that can be tailored for specific computational tasks. When combined with traditional CPUs, they can offer significant performance benefits in tasks that benefit from hardware-level customisation, such as encryption, signal processing, and certain machine learning operations.
- The Arm Exascale Programme Initiative (EPI) is an example of a project aiming to use CPUs in combination with FPGAs to create energy-efficient, high-performance exascale systems.

Although exascale supercomputers are under development, current petascale systems (which are one thousand times less powerful than exascale systems) still face limitations when it comes to fully exploiting their capabilities.

As the industry moves towards heterogeneous architectures that combine CPUs, GPUs, and FPGAs, the need for expertise in optimising HPC applications will continue to grow, offering lucrative job opportunities for those with the necessary skills.
