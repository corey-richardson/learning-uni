# Non-relational Databases

- [databases](#databases)
- [what-are-non-relational-databases](#what-are-non-relational-databases)
- [big-data](#big-data)
    - [challenges-of-big-data](#challenges-of-big-data)
        - [volume](#volume)
        - [velocity](#velocity)
        - [variety](#variety)
        - [veracity](#veracity)
        - [value](#value)
- [unstructured-data](#unstructured-data)
    - [in-memory-analytics](#in-memory-analytics)
- [ambiguity-of-big-data](#ambiguity-of-big-data)
- [ambiguity-of-nosql](#ambiguity-of-nosql)
- [nosql](#nosql)
    - [cluster-friendly-design](#cluster-friendly-design)
    - [open-source-philosophy](#open-source-philosophy)
    - [built-for-the-21st-century-web](#built-for-the-21st-century-web)
- [non-relational-data-models](#non-relational-data-models)
    - [document-oriented-database](#document-oriented-database)
    - [key-value-store](#key-value-store)
    - [graph-database](#graph-database)
    - [google-bigtable](#google-bigtable)
    
## Databases

> "A database is a collection of information that exists over a long period of time, often many years.", Garcia-Molina et al.

Databases are foundational tools in computing, designed to store, manipulate, and display information over extended periods, often spanning many years. 

Traditional databases have been predominantly relational (SQL-based), but non-relational databases (NoSQL) have gained popularity due to their flexibility, scalability, and ability to handle diverse data structures.

## What Are Non-Relational Databases?

Non-relational databases, often referred to as NoSQL databases, do not use the traditional table-based structure of relational databases. Instead, they adopt a variety of models tailored to specific use cases, such as documents, key-value pairs, graphs, or wide-columns.

Unlike relational databases, they do not rely on structured query language (SQL) or fixed schemas, making them well-suited for handling large-scale, unstructured, or semi-structured data.

## Big Data

> "Big data refers to datasets whose size is beyond the capacity of typical database software tools to capture, store, manage, and analyse.", John Morton (Chief Technology Officer, SAS UK)

As technology advances, the size of datasets qualifying as big data continues to grow, challenging conventional tools and methods for data management and analysis.

### Challenges of Big Data

#### Volume

- Describes the scale of data generated.
- From social media posts to satellite imagery, data production is growing exponentially.
- The sheer amount of data generated is immense, often measured in terabytes, petabytes, or even exabytes.
- Social media platforms like Twitter generate millions of tweets per minute. Even though each tweet is small, the combined dataset quickly grows beyond manageable limits.

#### Velocity

- Refers to the speed at which data is generated and processed.
- Real-time data streams (e.g., financial transactions, IoT devices, and web logs) require rapid ingestion and analysis.

#### Variety

- Highlights the diverse types of data from various sources.
- Big data encompasses data from a wide array of sources, formats, and structures:
    - Structured data (e.g., tables)
    - Semi-structured data (e.g., JSON, XML)
    - Unstructured data (e.g., text, images, videos, medical imaging)

#### Veracity

- Veracity refers to the messiness and uncertainty inherent in big data. 
- As data is generated from diverse, unstructured sources like social media, emails, and IoT devices, ensuring accuracy and reliability becomes more challenging.
- Modern technologies like machine learning and natural language processing (NLP) help process messy, unreliable data by identifying patterns, correcting inaccuracies, and making data usable for analysis.

#### Value
- Value is the ultimate goal of working with big data; it refers to the ability to extract actionable insights and create tangible benefits from data analysis.
- Aircraft engine manufacturers use big data to predict engine events with 97% accuracy, preventing costly airline disruptions. These insights save millions by avoiding unexpected downtime and improving operational efficiency.
- Technologies like in-memory analytics allow businesses to process and analyse data as it is generated, offering immediate insights and faster decision-making.

The relationship between data quality (veracity) and business outcomes (value) is crucial. Without reliable data, insights drawn from analysis may be misleading or even harmful.

- Implement data cleansing processes to improve data quality.
- Use robust analytics tools to identify and mitigate errors in real time.
- Balance quantity with quality; prioritise trustworthy datasets for critical decisions.

## Unstructured Data

Around 80% of the world's data is unstructured, encompassing text, voice, images, video, and other non-tabular formats. 

Unlike structured data (which fits neatly into databases), unstructured data requires advanced tools and techniques to extract meaningful insights.

In just one minute, the world produces:
- 72 hours of new multimedia material uploaded to YouTube.
- Over 200,000 Instagram posts, with images, videos, and captions.
- Over 200 million emails exchanged globally, often containing attachments, multimedia, and other diverse data formats.

As the volume of unstructured data explodes, traditional systems struggle to manage the storage and processing needs:
- Unstructured data lacks a predefined schema, making analysis and storage more complex.
- The growth rate far outpaces the ability of conventional methods to handle it effectively.

### In-Memory Analytics

A technology that allows data to be analysed while it is being generated. By storing data in RAM (rather than slower disk-based systems), processing times are drastically reduced, enabling near-instantaneous insights.

Real-time analysis enables businesses to process streams of data as they come in, facilitating quick decision-making.

Scalability handles large volumes of data efficiently, accommodating the needs of modern applications like IoT and e-commerce.

## Ambiguity of Big Data

Big data is a widely used yet ill-defined term, often characterised by a lack of precision in its definition. This ambiguity can lead to misunderstandings and inconsistent interpretations across industries and disciplines.

There is no universally accepted definition of big data. It's characteristics are often described through frameworks like the Vs (Volume, Velocity, Variety, etc.), but these are not prescriptive.

What qualifies as "big data" evolves with technological advancements. As tools and storage capacities improve, datasets that were once considered big data may no longer fall into that category.

## Ambiguity of NoSQL

NoSQL is another example of an accidental and ambiguous term, similar to big data.

"NoSQL" was not coined with a strict definition but emerged as a neologism to describe databases that deviate from traditional relational database principles.

> *neologism*, a newly coined word or expression.

The term encompasses a broad category of databases that are:
- Non-relational
- Horizontally scalable
- Schema-less (in many cases)

Flexibility
- They handle unstructured or semi-structured data.

Scalability
- Designed to scale out easily across distributed systems.

Variety of Models
- Includes document stores, key-value stores, graph databases, and column-family databases.

## NoSQL

NoSQLmeans *Not Only SQL*, highlighting the flexibility to use alternative data storage mechanisms alongside or instead of traditional relational databases. 

This approach acknowledges that different software solutions or products may require different types of databases depending on the use case.

NoSQL databases are designed to handle unstructured, semi-structured, or structured data, offering a variety of data models beyond the rigid structure of relational databases.

NoSQL implies that SQL-based relational databases are not the only option for managing data. Developers can choose storage mechanisms that best suit their application's needs.

### Cluster-Friendly Design

NoSQL databases are built to work seamlessly in distributed environments.

Data is stored across multiple nodes in a cluster rather than a single location.

Horizontal scaling allows you to add more servers to the cluster improves performance and storage capacity.

### Open Source Philosophy

Many NoSQL systems are open source, meaning they are freely available to use and modify. Instead of traditional customer support, users rely on community-driven resources such as forums, GitHub repositories, and Stack Overflow.

Cost-effective for startups and small-scale projects. Benefit from a large and active community for troubleshooting and innovation.

Limited official support for enterprise-grade deployments (compared to proprietary systems).

### Built for the 21st Century Web

NoSQL databases are typically associated with modern web applications and Internet-based systems.

Supports real-time analytics. Handles high concurrency loads. Manages dynamic, ever-evolving data structures.

## Non-Relational Data Models

NoSQL databases are not table-based, unlike traditional relational databases.

- Document Stores (e.g., MongoDB)
- Key-Value Stores (e.g., Redis)
- Graph Databases (e.g., Neo4j)
- Column-Family Stores (e.g., Cassandra)

Avoids rigid schemas, making it easier to adapt to changes in data structure. Better suited for unstructured or semi-structured data.

### Document-Oriented Database

Stores data as documents (e.g., JSON, BSON, or XML) rather than rows and columns. Each document is self-contained and may have a unique structure.

Schema-less design allows flexibility for rapidly changing data models. Supports complex queries within documents.

Used for content management systems, e-commerce platforms, and IoT applications.

- MongoDB: Popular for its flexibility and powerful query capabilities.
- CouchDB: Focused on distributed, multi-version document storage.
- OrientDB: Combines document and graph capabilities.
- RavenDB: Optimised for .NET environments.
- Lotus Notes: Early pioneer in document-based storage.

### Key-Value Store

A simple database model where data is stored as key-value pairs, making it highly efficient for fast lookups.

Data is accessed via unique keys, which map to specific values. Ideal for applications requiring low-latency reads and writes.

Used for session management, caching, and real-time analytics.

- Redis: Known for in-memory storage and high performance.
- MemcacheDB: Used for caching frequently accessed data.
- Berkeley DB (BDB): Lightweight, embedded database.
- HamsterDB: A simple and fast alternative for embedded systems.

### Graph Database

A database designed to store and manage data represented as nodes (entities) and edges (relationships).

Efficiently handles complex relationships between data points. Queries are optimised for graph traversal rather than table joins.

Used in social networks, recommendation engines, fraud detection, and network analysis.

- Neo4j: The most widely used graph database, designed for scalable and high-performance graph processing.
- HyperGraphDB: Focused on knowledge representation and semantic querying.

### Google Bigtable

A scalable, distributed storage system designed for structured data, built by Google.

Data is stored in rows and columns but is schema-flexible. Optimised for large-scale applications, such as web indexing and analytics. Underpins many Google services, including Google Maps and Google Earth.
