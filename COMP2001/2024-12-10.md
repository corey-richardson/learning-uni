# Docker

- [why-docker](#why-docker)
    - [benefits-of-docker](#benefits-of-docker)
    - [docker-vs-traditional-virtual-machine](#docker-vs-traditional-virtual-machine)
    - [docker-vs-windows-containers](#docker-vs-windows-containers)
- [docker-overview](#docker-overview)
- [docker-architecture](#docker-architecture)
    - [the-docker-runtime](#the-docker-runtime)
    - [the-docker-daemon-dockerd](#the-docker-daemon-dockerd)
    - [docker-cli-command-line-interface](#docker-cli-command-line-interface)
    - [docker-api](#docker-api)
    - [the-orchestrator](#the-orchestrator)
    - [docker-registry](#docker-registry)
    - [docker-images](#docker-images)
    - [containers](#containers)
    - [new-docker-architecture](#new-docker-architecture)
- [basic-usage](#basic-usage)
- [docker-workflow-example](#docker-workflow-example)
- [starting-a-new-container](#starting-a-new-container)
- [images-and-containers](#images-and-containers)
- [steps-to-containerize-an-application](#steps-to-containerize-an-application)

---

## Why Docker?

Traditionally, each application was deployed on a separate server. This meant using one application per server and requiring big, fast servers to handle each workload individually.

As applications grew in size and complexity, the inefficiency of this approach became apparent. Servers often went underutilised, resulting in wasted resources, higher costs, and a larger environmental footprint.

Virtual Machines allowed running multiple applications on a single physical server by virtualising hardware. Each VM has its own dedicated operating system (OS), which allows for greater flexibility in managing different applications.

- Every VM requires its own OS, leading to higher resource consumption (CPU, RAM, storage).
- VMs are slow to boot and can be resource-intensive because of the overhead caused by running multiple OS instances on the same host.

Docker is an alternative to VMs that provides a more lightweight and efficient way to run applications in isolated environments called *containers*.

Lightweight Containers
- Containers don't require their own OS like VMs. Instead, they share the host system's OS kernel but run in isolated user spaces.
- This reduces the overhead (memory and CPU) compared to VMs and makes containers much more efficient in terms of resource usage.
- Containers are faster to start, as there's no need to boot an entire operating system.

Reduced Licensing and Maintenance Costs
- Since containers share the host OS kernel, you don't need to worry about licensing for multiple OS instances, nor do you need to manage OS patching or updates for every instance, as you would with VMs.
- This leads to cost savings on both the infrastructure and operational side.

Simple Setup and Management
- Containers solve the complexity of managing Linux Containers (LXC), which were traditionally difficult to set up and manage. Running a simple application like a web server inside an LXC could require:
    - Setting up control groups and namespaces manually.
    - Installing and configuring packages in the container.
    - Managing filesystem isolation and networking manually.

    Docker abstracts these complexities and allows you to start containers with simple commands.

    ```bash
    docker run -p 80:80 nginx
    ```

    > This single command runs an Nginx web server inside a container and maps port 80 on the container to port 80 on the host system. Docker handles all the isolation, package management, and networking for you, making it significantly easier to deploy and manage applications.

Consistency Across Environments
- Docker containers ensure that an application runs the same way, regardless of where the container is deployed (on a developer's laptop, on a test server, or in production).
- This helps eliminate environmental inconsistencies, making it easier to move applications between different stages of development.

### Benefits of Docker

Portability
- Containers can run anywhere (laptops, data centres, cloud services) without modification, because the environment and dependencies are packaged along with the app.

Efficiency
- Containers use fewer resources than VMs, allowing you to run more applications on the same hardware.

Speed
- Containers start almost instantly, in contrast to VMs, which can take several minutes to boot.

Isolation
- Each container runs in its own environment, ensuring that it does not interfere with other containers or applications on the same system.

Scalability
- Docker makes it easy to scale applications, as containers can be spun up and down quickly to meet demand.

### Docker vs Traditional Virtual Machine

If you wanted to run a web server on a VM, you would need:
- A complete OS instance (e.g., Ubuntu).
- A hypervisor to manage the VM.
- Significant resources to run the OS and web server.

Using Docker, you can run the same web server in a container:
- No need for a full OS; just the necessary application and dependencies.
- Containers share the host OS kernel, making it lighter and faster than a VM.

### Docker vs Windows Containers

Windows containers run Windows apps that require a host system with a Windows kernel.

Any Windows host running the Windows Subsystem for Linux (WSL) can also run Linux containers.

Most containers are Linux containers because they are smaller and faster.

Windows containers are essential for running Windows-based applications in a containerised environment, especially in scenarios where a Windows kernel is required. While Linux containers are more commonly used due to their smaller size and faster performance, Windows containers play a key role in hybrid systems and enterprise environments that rely on Windows applications. Docker Desktop on Windows makes it possible to run both Windows and Linux containers on a single machine, offering flexibility for developers working across different platforms.

## Docker Overview

Docker is a powerful platform used for containerisation, enabling developers to package applications and their dependencies into containers that can run consistently across various environments (such as development, testing, and production). Docker runs on Linux, Mac, and Windows and is built from tools provided by the Moby open-source project.

## Docker Architecture

### The Docker Runtime

This is the engine responsible for running containers. It starts, stops, and manages the life cycle of containers.
The runtime can run containers in both isolated environments and on clusters, enabling the efficient use of resources.

### The Docker Daemon (dockerd)

The Docker daemon is the background service running on a system that is responsible for:
- Starting and stopping containers.
- Managing container images (downloading, building, storing, etc.).
- Managing networks and volumes associated with containers.
- Exposing APIs that allow communication with the Docker engine.

The Docker daemon listens for Docker API requests and performs actions such as creating and managing containers or images.

> A daemon (pronounced "dee-muhn") is a background process that runs continuously on a system, typically without user interaction. It is designed to perform specific tasks or handle certain operations on behalf of the operating system or applications. Daemons are usually started when the system boots up and run in the background to provide services, often without user involvement.

### Docker CLI (Command Line Interface)

The CLI allows users to interact with the Docker daemon through commands like docker run, docker build, docker ps, etc.
It provides a command-line interface for managing containers, images, networks, and volumes.

### Docker API

The Docker API is exposed by the daemon and allows applications, services, or users to interact programmatically with the Docker engine.

It provides a set of HTTP endpoints that can be used to create, manage, and interact with containers and images.

### The Orchestrator 

- Kubernetes, Docker Swarm. etc

Orchestration is essential when managing multiple containers or clusters.

Docker itself provides a basic orchestrator called Docker Swarm, but it can also integrate with more complex orchestrators like Kubernetes.

An orchestrator manages the deployment, scaling, and operation of containers across a cluster of machines, ensuring high availability and load balancing.

### Docker Registry

A Docker registry stores container images. The most well-known registry is Docker Hub, but users can also set up private registries for their own images.

Registries allow Docker images to be stored and shared between different users or systems.

### Docker Images

Images are the blueprints from which containers are created. A Docker image is a read-only template that contains the application code, libraries, and dependencies needed to run the application.

Images are built using Dockerfiles and can be pulled from a registry or built locally.

### Containers

Containers are the running instances of Docker images. They are lightweight, portable, and isolated from the host environment, which allows them to run consistently across different systems.

Containers share the host operating system's kernel but are isolated in terms of file systems, networking, and process trees.

### New Docker Architecture

`runc`
- `runc` is a lightweight command-line interface (CLI) wrapper for libcontainer that creates containers.
It is a standardized container runtime and serves as the low-level tool for creating and managing containers.

`containerd`
- containerd is a core component that manages container lifecycle and execution logic. It handles the full container lifecycle, including image handling, container creation, and execution.
It decouples the high-level Docker features from the container runtime itself, ensuring greater flexibility and scalability.

`shim`
- The `shim` is responsible for decoupling the execution of containers from the Docker daemon. It allows containers to run independently of the Docker daemon, enabling daemonless container management.
- The `shim` acts as an intermediary between the container runtime and the Docker daemon, helping in managing containers without relying on the Docker daemon for every action.

## Basic Usage

Docker can be used both for operations (Ops) and development (Dev):

Ops
- Start and manage containers, including running commands and logging in.
- Manage lifecycle: create, start, stop, destroy containers.

Dev
- Use Dockerfiles to define how images are built.
- Run containers from those images in different environments for development and testing.

## Docker Workflow Example

1. Create an image using a `Dockerfile`.
2. Build the image using the Docker CLI.
3. Push the image to a Docker registry (like Docker Hub).
4. Pull the image from a registry.
5. Run a container from the image, using the Docker runtime.
6. Manage and scale containers using orchestration tools.

## Starting a New Container

When starting a new container using Docker, a sequence of events takes place to manage and execute the container.

```bash
docker container run
```

1. Issue `docker container run` Command

When you issue the `docker container run` command from the Docker CLI, Docker communicates with the Docker daemon (`dockerd`), which is a long-running background process that manages Docker containers.

The daemon exposes a REST API, and the Docker CLI makes an API call to this endpoint.

2. API Call to Docker Daemon

The Docker daemon receives the request via its exposed API endpoint and begins processing the command to start a new container.

The daemon interacts with `containerd`, a core component that is responsible for managing containers.

3. Containerd Receives the Instruction

Containerd is a high-level container runtime responsible for managing the container lifecycle, including creating, starting, and stopping containers.

Docker instructs `containerd` to create a new container based on the image specified in the `docker run` command.

4. gRPC API Communication

`containerd` interacts with runc (a low-level container runtime) to create the container.

This communication happens through a gRPC API, which allows containerd to tell runc to create and start a container.

5. Container Creation

runc takes over the actual creation and execution of the container. It sets up the container's namespaces (e.g., filesystem, network), cgroups, and other kernel-level configurations to isolate the container from other processes running on the host system.

Once the container is created and the configuration is applied, runc starts the container.

6. runc Exits After Container Starts

After the container has been successfully started, runc exits because its job is to only create and start the container, not to manage it once it's running.

7. Shim Becomes the Parent Process

Shim is a lightweight process that acts as the parent process of the container once runc has started it.

The shim's role is to handle the container’s lifecycle after it has been started. It also provides a way for the Docker daemon to interact with the container (e.g., for logging or stopping the container).

The shim ensures that the container's output is handled, and it allows communication between the container and the Docker daemon.

8. Container Running

At this point, the container is running, and it operates in isolation, using the resources allocated by Docker.

The shim continues running as the container’s parent process, and the container itself is handled by the kernel through namespaces and cgroups.

## Images and Containers

Container
- A container is a runtime instance of an image. It is an encapsulated and isolated environment that is created from an image and can be executed, modified, and deleted. A container includes everything needed to run an application: the code, runtime, libraries, and dependencies.

Image
- An image is a lightweight, stand-alone, and executable package that includes everything required to run an application. It is a read-only template used to create containers. Images are stored in registries (e.g., Docker Hub) and are downloaded or pushed to these registries as needed.

Relationship
- Containers depend on images to be created, and an image is not useful until it is run as a container. Images are essentially the blueprint, and containers are the active, running instances.

Registries
- Docker images are stored in registries like Docker Hub, Google Container Registry, or private registries. These registries allow developers to share, store, and download images.

## Steps to Containerize an Application

1. Start with your application code and dependencies
2. Create a Dockerfile
3. Build it into an image
4. Push the image to a registry (optional)
5. Run a container from the image

Begin with the source code and any required dependencies for your application. For example, a web app might need a specific version of a web framework or database libraries.

A *Dockerfile* is a text file that contains the instructions on how to build a Docker image. It defines the base image to use, adds application files, installs dependencies, and specifies how the container should run the application.

```dockerfile
# Dockerfile for a simple Python app

# Use a base image with Python installed
FROM python:3.8-slim

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container
COPY . /app

# Install any dependencies
RUN pip install -r requirements.txt

# Set the default command to run the app
CMD ["python", "app.py"]
```

Once the Dockerfile is ready, you can build the image.

```bash
docker build -t myapp:latest .
```

This command tells Docker to build an image using the current directory (`.`) and tag it as `myapp:latest`.

You can push your image to a Docker registry like Docker Hub so that it can be shared with others or used on different systems.

```bash
# Log in to Docker Hub
docker login
# Tag the image with your Docker Hub repository name
docker tag myapp:latest username/myapp:latest
# Push the image to the registry
docker push username/myapp:latest
# This makes the image available for others to download and run
```

Once the image is built, you can run a container from the image.

```bash
docker run -d -p 8080:8080 myapp:latest
```

This starts a container in the background (`-d` flag) and maps port 8080 on your host to port 8080 in the container (`-p 8080:8080`), allowing you to access the app in the container via `localhost:8080`.
