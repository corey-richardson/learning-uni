# COMP1001 Computer Systems
# 2023-11-13 Memory Hierarchy and Cache

---

- Memory Hierarchy
- Cache Memories
- Cache Design
    - Cache Hit/Miss
    - Direct Mapped, Set Associative, Fully Associative
    - Replacement Policy

---

![](https://runestone.academy/ns/books/published/welcomecs2/external/ComputerArchitecture/Images/Memory-Hierarchy.jpg)

![](https://www.researchgate.net/profile/Bojan-Jovanovic-6/publication/281805561/figure/fig1/AS:324966131224576@1454489371431/Typical-structure-of-a-computer-memory-hierarchy.png)

Wouldn't it be nice if we could find a balance between fast and cheap memory?
- The solution is to add from 1 up to 3 levels of cache memories, which are small, fast, but expensive memories
    - The cache goes between the processor and the slower, main memory (DDR)
    - It keeps a copy of the most frequently used data from the main memory
    - Faster reads and writes to the most frequently used addresses
    - We only need to access the slower main memory for less frequently used
data
- `-` Cache memories occupy the largest part of the chip area
- `-` They consume a significant amount of the total power consumption
- `-` Add complexity to the design

Cache memories are of key importance regarding performance

---

What the CPU needs to perform a load instruction:
- First it looks at L1 data cache. If the datum is there then it loads it and no other memory is accessed (*L1 hit*)
- If the datum is not in the L1 data cache (*L1 miss*), then the CPU looks at the L2 cache
- If the datum is in L2 (*L2 hit*) then no other memory is accessed.
- Otherwise (*L2 miss*), the CPU looks at main memory

Type     | Access Time (Cycles)
---      | --- 
L1 cache | 1-4 CPU cycles
L2 cache | 6-14 CPU cycles
L3 cache | 40-70 CPU cycles
DDR      | 100-200 CPU cycles

---

```cpp
// Row wise
// Many caches hits
for (int i = 0; i < 1024; i++) {
    for (int j = 0; j < 1024; j++) {
        A[i][j] = ...
    }
}
```
```cpp
// Column wise
// Many cache misses
for (int j = 0; j < 1024; j++) {
    for (int i = 0; i < 1024; i++) {
        A[i][j] = ...
    }
}
```

> The first code can be up to x16 times faster than the second on a common PC.

---

## Memory Allocation for 1D Arrays

![](/learning-uni/COMP1001/res/2023-11-13_1D_Arrays.png)

## Memory Allocation for 2D Arrays

![](/learning-uni/COMP1001/res/2023-11-13_2D_Arrays_a.png)
![](/learning-uni/COMP1001/res/2023-11-13_2D_Arrays_b.png)
![](/learning-uni/COMP1001/res/2023-11-13_2D_Arrays_c.png)

---

## Cache Design

Caches are divided into blocks, which may be of various sizes. The number of blocks in cache memories are always in power of 2.

A direct-mapped cache is the simplest approach: each main memory address maps to exactly one cache block.


Memory locations 0, 4, 8 and 12 all map to cache block 0

Addresses 1, 5, 9 and 13 map to cache block 1, etc

$$\text{block} = \text{location} \, \% \, n$$

An equivalent way to find the placement of a memory address in the cache is to look at the least significant k bits of the address.

![](/learning-uni/COMP1001/res/2023-11-13_CacheBlocks_a.png)
![](/learning-uni/COMP1001/res/2023-11-13_CacheBlocks_b.png)
![](/learning-uni/COMP1001/res/2023-11-13_CacheBlocks_c.png)
![](/learning-uni/COMP1001/res/2023-11-13_CacheBlocks_d.png)

Initially, the cache is empty and does not contain valid data, but trash.

Thus, we add a *valid bit* for each cache block
- When the system is initialized, all the valid bits are set to 0
- When data is loaded into a particular cache block, the corresponding *valid bit* is set to 1

![](/learning-uni/COMP1001/res/2023-11-13_CacheBlocks_e.png)

## Cache Hit

When the CPU tries to read from memory, the address will be sent to the cache controller.

The lowest `k` bits of the address will index a block in the cache.

If the block is valid and the tag matches the upper (`m - k`) bits of the `m`-bit address, then that data will be sent to the CPU.

![](/learning-uni/COMP1001/res/2023-11-13_CacheHit.png)

## Cache Miss

The simplest thing to do is to stall the pipeline until the data from main memory can be fetched (and also copied into the cache).

In a two level memory hierarchy, L1 Cache misses are somehow expensive, but L3 cache misses are very expensive.

However, the slower main memory accesses are inevitable on an L3 cache miss.

## Full Cache

Eventually, the small cache memory might fill up.

To load a new block from DDR, weâ€™d have to replace one of the existing blocks into the cache... which one?

- A miss causes a new block to be loaded into the cache, automatically overwriting any previously stored data
- Normally, the least recently used (*LRU*) replacement policy is used, which assumes that least recently used data are less likely to be requested than the most recently used ones
- So, in a cache miss, cache throws out the cache line that has been unused for the longest time

## Associativity

...
