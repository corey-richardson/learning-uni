# COMP1001 Computer Systems
# 2023-10-16 Instruction Pipelines and Hazards

---

> "Fetch, Decode, Execute" <br>
> <sub>- Control Unit</sub>

Each execution step uses a different functional unit. There's a lot of hardware doing nothing.

We shouldn't wait for an entire instruction to complete before we can use the other function units.

Pipelining solves this inefficiency.

---

![](/COMP1001/res/2023-10-16_01.png)
![](/COMP1001/res/2023-10-16_02.png)

- 1-3 Filling
- 4 Full
- 5-7 Emptying

The *latency* of a single load remains 4 hours but *throughput* is increased (1 completed per 1 hour); the number of loads completed per unit of time.

---

Instructions are divided into smaller tasks which are performed by different processor units.

Pipelining increases the CPU instruction throughput; the number of instructions completed per unit of time.

It does not reduce the execution time of an individual instruction. In fact, it usually slightly increases the execution time of each instruction due to *overhead* in the pipeline control.

Pipeline overhead arises because extra hardware is needed (registers) which introduce a delay for several reasons, e.g., *clock skew*.

Clock skew is a phenomenon in synchronous circuits in which the clock signal arrives at different components at different times.

![](https://www.allaboutcircuits.com/uploads/articles/Clock-Skew-2.jpg)

---

> Consider a non-pipelined machine with 5 execution stages of lengths 50ns, 50ns, 60ns, 60ns and 50ns.

Instruction Latency:
$$50+50+60+60+50 = 270\text{ns}$$

Time to exectute 100 instructions:
$$100*270 = 27,000\text{ns}$$

> Suppose we introduce pipelining. Assume the clock skew adds 5ns of overhead to each execute stage.

Instruction Latency: <br>
<sub>The length of the pipe stages must all be the same; speed of the slowest stage plus the overhead.</sub>
$$60+5 = 65\text{ns}$$

Time to execute 100 instructions:
$$65*5*1 + 65*1*99 = 325+6435 = 6760\text{ns}$$

Speed-up:
$$\frac{27,000}{6760} = 3.994 \approx4$$

## Five Stage RISC Pipeline

1. *Instruction Fetch*: Read an instruction from memory
- The instruction is read from memory
- The instruction is loaded from L1 instruction cache (1 cycle)
    - But the next instruction is not always in the instruction cache
- The instruction is stored into the Instruction Register (IR)

2. *Instruction Decode*: identify the instruction and its operands
- The processor reads the Instruction Register (IR) and identifies the instruction
- Reads any operands required from the register file
- The CPU generates the control signals
- The instruction decode phase will calculate the next PC and will send it back to the IF phase so that the IF phase knows which instruction to fetch next

3. *Execute*: execute an arithmetical instruction or compute the address of a load/store
- The Execute stage is where the actual computation occurs
- These calculations are all done by the ALU
- The arithmetical instructions are executed at this stage
- For load/store instructions, the address calculation is made

4. *Memory*: load or store from/to memory
- The Memory Access stage performs any memory access required by the current instruction
- So, for loads, it would load an operand from L1 data cache memory
- For stores, it would store an operand into memory
- For all other instructions, it would do nothing
- Note that the data are not always in L1 data cache memory

5. *Write Back*: Store the result in the destination register
- For instructions that have a result (a destination register), the Write Back writes this result back to the register file

> *Not all instructions need all five stages.*

Instruction | Steps required
--- | --- 
Arithmetical | IF ID EX *NOP* WB
Load | IF ID EX MEM WB
Store | IF ID EX MEM *NOP*
Branch | IF ID EX *NOP* *NOP*

*NOP* - No operation

---

Pipelining attempts to maximize instruction throughput by overlapping the execution of multiple instructions.

In the best case, one instruction finishes on every cycle, and the speedup is equal to the pipeline depth.

---

## Hazards

Hazards prevent next instruction from executing during its designated clock cycle.

1. Structural hazards: HW cannot support the usage of a function unit to 2 instructions at the same time
2. Data hazards: Instruction depends on result of prior instruction still in the pipeline
3. Control hazards: Pipelining branch and jump instructions introduce the problem that the destination of the branch is unknown

Common solution is to *stall* the pipeline until the hazard is resolved, inserting one or more NOP cycles in the pipeline.

```asm
add R1, R2
sub R5, R1
```
> Sub instruction, needs the value of register R1 that will be available after the Write Back (WB) stage

![](/COMP1001/res/2023-10-16_03.png/)

`Instr2::ID` requires `Instr1::WB` to be completed, so it awaits this process being completed.

## Pipeline Forwarding / Bypassing

The results of `R1` is ready after the `Instr1::EX` stage, so it is forwarded.

![](/COMP1001/res/2023-10-16_04.png/)

> Only 1 `stall` cycle instead of 3

[Related Video](https://www.youtube.com/watch?v=EW9vtuthFJY)

## Control Hazards

A control hazard is when we need to find the destination of a branch, and can't fetch any new instructions until we know that destination.

Branch Prediction: The outcome and target of conditional branches are predicted using some heuristic.
- Branch predictors play a critical role in a modern CPU

Without branch prediction, the processor would have to wait until the conditional jump instruction has passed the execute stage before the next instruction can enter the fetch stage in the pipeline

The branch predictor attempts to avoid this waste of time by trying to guess whether the conditional jump is most likely to be taken or not taken.
- The branch that is guessed to be the most likely is then fetched and *speculatively executed*.
- If it is later detected that the guess was wrong then the speculatively executed or partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.

**heuristic**
> proceeding to a solution by trial and error or by rules that are only loosely defined.

- [ ] Complete Hardware Architecture questions on DLE

---

## Loop Unroll Transformation

> Why assembly is useful to understand

```c
for (int i = 0; i < 100; i++) {
    A[i] = B[i];
}
```
```c
for (int i = 0; i < 100; i+=4) {
    A[i] = B[i];
    A[i+1] = B[i+1];
    A[i+2] = B[i+2];
    A[i+3] = B[i+3];
}
```

- `+` Reduces number of instructions
- `+` Increases instruction parallelism
- `+` The number of ASM `add` arithmetical instructions is reduced
- `+` Less ASM `cmp` compare instructions
- `+` Less ASM  `jmp` jump commands
- `-` Increases code size 
- `-` Increases register pressure