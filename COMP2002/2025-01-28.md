# Machine Learning

- Introduction to Machine Learning
- Classification Methods
- Clustering Methods
- *Reinforcement Learning*

---

## Varieties of Machine Learning

Supervised Learning
- Training Models (known target values)

Unsupervised Learning
- Finding structure in data (no target values)

## Machine Learning Tasks

Classification 
- work out which of a set of categories an object belongs to

Clustering 
- identify groupings occuring within the data

Regression 
- predict a single numerical value

## Classification

Challenges
- Sparse data
- Overlapping classes
- Uneven classes (bias towards classes with more datapoints)
- Often not linearly seperable

## Iris Dataset

Cluster according to
- sepal width and height
- petal width and height

Labelled Dataset, 150 observations on 4 features, 3 classes of 50 observations each, 1 class is linearly seperable and the 2 others are not.

## Dimension Reduction

Principal Component Analysis

Reducing the dimensionality (number of variables) of the data by discovering redundant data.

Feature Selection
- Identify redundant features and discard them

Feature Extraction
- Find a new set of low-dimensional points that represents the orginal data well

Preserve the characteristics of the original data.

PCA is a technique to reduce the dimensions of a $n * p$ data matrix $X$.

The first prinipal component is in the direction of the data along which there is the greatest amount of variation in the data.

The second principal component is a linear combination of the variables that is uncorrelated with the first principal component  subject to the constraint that is has the largest variance.

![](https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2023/01/PCA_original.png?fit=596%2C462&ssl=1)


## Eigenvectors and Eigenvalues


Eigenvalues are the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component.

Ranking eigenvectors in the order of their eigenvalues gives you the most significant principal components.

## K-Nearest Neighbours (KNN)

Given a set of points with known classes, and a new point of an unknown class.

Find the $k$ nearest known points to the unknown point.

Assign the class held by the majority of the $k$ points to be the class of the unknown point.

The clusters do not overlap.

If there are an equal number of nearest classes, weight the distances. Or randomise the choice.

Plot the training error, showing number of considered neighbours against the error rate.

## Decision Trees

Can be applied to both regression and classification problems.

- Trees are very easy to explain.
- Some people believe they mirror human decision making
closely
- Trees can be displayed graphically.
- Trees can easily handle qualitative predictors without the
need to create dummy variables. <br><br>

- Trees do not have the same level of predictive accuracy as
other methods.
- Trees can be very non-robust.

## Random Forests

A set of decision trees working together - *ensemble learning*.

Each tree is constructed from a random set of features - each tree is different.

Unlike decision trees, each time a split is considered only a random sample of the predictors is considered.

We build a number of different decision trees.

At each split the algorithm is allowed to use only one of the $m$ predictors.

$$m \approx \sqrt{p}$$

This reduces the probability of a strong predictor being used in all of the trees, as on average only...

$$\frac{p-m}{p}$$

...of the splits will not even consider the strong predictor.

This results in the average of the trees having less variability and therefore more reliable.

## Support Vector Machines (SVM)

Generalisation of a simple and intuitive classifier called maximal margin classifier.

Flexible, good performance compared to other classifiers.

![](https://d1rwhvwstyk9gu.cloudfront.net/2019/10/Understanding-Support-Vector-Machines-with-an-example-3.png)

First we decide the shape the class boundary should have:

- linear
- polynomial
- radial
- sigmoid

This is the Kernel of the SVM.

Second, we decide how wide the so-called margin should be.

- The margin defines the area in the direct neighbourhood of class boundary.
- Only the data points inside the margin area will be taken into account when forming the class boundary.
- These data points are called support vectors.
- The width of the margin is specified indirectly via the so-called tuning parameter.

In the last step, the SVM algorithm determines the class boundary by finding the line or curve, depending on step 1, that separates the support vectors in an optimal way.

## Confusion Matrices

Compares predictions against targets.

Heatmap plot.

High value (yellow/hot) in diagonal entries indicates high accuracy.

![](https://user-images.githubusercontent.com/45271643/59080393-dded7000-88be-11e9-9465-c145845cbaac.png)

- Accuracy
- Recall (Sensitivity)
- Precision
- $f_1$ score (Combination of Precision and Recall)

## ROC Curve

Receiver Operating Characteristics Curve

Displays two types of errors simultaneously.

Overall performance is given by the area under the curve.

![](https://www.researchgate.net/publication/356259132/figure/fig3/AS:1117052731432967@1643337533932/ROC-curve-of-training-and-test-set.png)

## Clustering

### K-Means Clustering

- Require $X$ - a set of $n$ points to be clustered.
- Require $k$ - number of clusters required.

1. Select a set of $k$ cluster centres $c_j$ where $j = 1, \dots, k$
2. while not converged, do
3. Allocate each member of X to their nearest $c_j$
4. Replace each $c_j$ with the new mean of cluster $j$
5. end while

![](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png)
