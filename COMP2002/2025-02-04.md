# Neural Networks

- Regression
- Neural networks
- Regression and classification problems with neural networks
- Neural networks in Scikit

---

## Regression

- Predicting a continuous or quantitative output value.
- Can be used to predict non-numeric or categorical/qualitative output using logistic regression.
- Linear regression is most common and simplest form.
- Least Squares approach

$$y \approx \beta_0 + \beta_1 x$$

$\beta_0$ and $\beta_1$ are model coefficients, estimated using the training data.

$$\hat{y} \approx \hat{\beta}_0 + \hat{\beta}_1 x$$

Frequentist and Bayesian methods are used.

We can have $p$ predictors:

$$y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_{p} x_{p} + \epsilon$$

$p$ values are used to check the influence of a variable on a model.

- RSS: Residual Sum of Squares
- RES: Residual Standard Error
- $R^2$

$$RSS=\sum^n_{i=1}(y_i - \hat{y})^2$$

$$RES = \sqrt{\frac{1}{n - 2} RSS }$$

$$R^2 = \frac{RSS}{\sum(y_i - \hat{y})^2}$$

## Neural Networks: Perceptron

$$y = f(xw)$$

- Algorithm for supervised learning.
- Model input is the $x$ variable.
- The model has a single unit.
- Input is connected to the unit by a weight, $w$.
- Output is a function of the weighted sum of input.

![](https://4.bp.blogspot.com/-sLmcD8myRC4/WjbfTsIFATI/AAAAAAAAAhE/Wmdq1A58Jzg5HpospH_9FR9-eFQvEjxGQCEwYBhgL/s1600/Perceptron.jpg)

- Multiple inputs; output is now the sum of the weighted inputs.
- Feed forward network
- $K$ is the number of inputs

$$y = f(\sum^K_{K=1} x_k w_k)$$

## Activation Functions

- Identity $f(a) = a$
- Sigmoid $f(a) = \frac{1}{(1+e^{-a})}$
- ReLU (Rectified Linear Unit) $f(a) = \max(0, a)$

Decide based on:
- functions affect on the learning process
- what activation function do you want to apply to a specific layer of units

## Measuring Error

MAE: Mean Absolute Error

## XOR

XOR (exclusive or) is a simple logic problem
- output value depends on the two inputs.

Minsky and Papert showed in the 1960s that this simple problem cannot be modelled by a SLP.

Instead we need multiple layers which can model the non-linearity.

## Multi-Layer Networks

![](https://media.geeksforgeeks.org/wp-content/uploads/nodeNeural.jpg)

## Generalisation

The model is trained to follow the training data. If it follows it too closely, it does not generalise the data. Prevent this with early stopping.

Overfitting is better than underfitting; ...ish.

Cross Validation assesses whether the model generalises. Does the model work on unseen data?

Train the model on a percentage of the data, $p$.

Test the model on $(100 - p)\%$ of the data; this data is unseen during training. `test_train_split`.
